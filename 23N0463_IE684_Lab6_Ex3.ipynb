{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Observe that the regularized problem (2) can be written as\n",
        "min\n",
        "x\n",
        "fλ(x) = min\n",
        "x\n",
        "XN\n",
        "i=1\n",
        "fi(x) (3)\n",
        "Find an appropriate choice of fi(x)."
      ],
      "metadata": {
        "id": "TmgGcjgKWdPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer-1\n",
        "\n",
        "We can write f(x) in the  form of $f_i(x)$, we can let:\n",
        "$$\n",
        "f_i(x)=\\frac{1}{2}\\left\\|a_i^T x-y_i\\right\\|^2+\\frac{\\lambda}{2N} x^Tx\n",
        "$$\n",
        "\n",
        "$ min_x f_\\lambda(x)=min_x\\sum_{i=1}f_i(x)$\n",
        "\n",
        "\n",
        "so,\n",
        "\n",
        "Gradient will be\n",
        "\n",
        "$$\n",
        " \\nabla{f_i(x)}= \\sum_{i=1}^n(a_i^T x-y_i)a_i + \\lambda x\n",
        "$$"
      ],
      "metadata": {
        "id": "f3axsKOQij4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write an expression to compute the gradient of fi(x) and denote it by gi(x) = ∇xfi(x)."
      ],
      "metadata": {
        "id": "avq_YBl7YNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer-2\n",
        "\n",
        "We already calculated gradient above, so expression to compute the gradient of fi(x) and denote it by is-\n",
        "\n",
        "$$\n",
        "g_i(x) = \\nabla{f_i(x)}=\\nabla_x{ ( \\frac{1}{2}\\left\\|a_i^T x-y_i\\right\\|^2+\\frac{\\lambda}{n} x^Tx )}\n",
        "$$\n",
        "$$\n",
        " = (a_i^Tx - y_i)a_i + \\frac{\\lambda}{n}x\n",
        "$$"
      ],
      "metadata": {
        "id": "XZcIEgCNkqnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Consider the dimension where you observed failure in the previous exercise. Implement the following algorithm\n",
        "(ALG-LAB6) to solve (3):"
      ],
      "metadata": {
        "id": "YouLrCk-YvGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import time\n",
        "import timeit\n",
        "\n",
        "np.random.seed(1000) #for repeatability\n",
        "N = 200\n",
        "d = 20000 #in the last queztion failue occure ar 20000\n",
        "lambda_reg = 0.001\n",
        "eps = np.random.randn(N,1)\n",
        "A = np.random.randn(N,d)\n",
        "#Normalize the columns\n",
        "for j in range(A.shape[1]):\n",
        "  A[:,j] = A[:,j]/np.linalg.norm(A[:,j])\n",
        "xorig = np.ones( (d,1) )\n",
        "y = np.dot(A,xorig) + eps\n"
      ],
      "metadata": {
        "id": "mXO1EvgZmicR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fx(x, lamda):\n",
        "  return (1/2)*norm(A@x-y)**2 + (1/2)*lamda*np.dot(x,x)\n",
        "\n",
        "def grad_fx(x, lamda):\n",
        "  sum = np.array([0. for _ in range(d)])\n",
        "  for i in range(N):\n",
        "    sum = sum+(A[i]@x - y[i])[0]*A[i]\n",
        "  sum =sum +  lamda*x\n",
        "  return sum"
      ],
      "metadata": {
        "id": "xrhjtxtnuPD6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((d,1))\n",
        "x = x.flatten()\n",
        "epochs = 10**4\n",
        "t = 1\n",
        "arr = np.arange(N)\n",
        "start = timeit.default_timer()\n",
        "for epoch in range(epochs):\n",
        "  np.random.shuffle(arr)\n",
        "  for i in np.nditer(arr):\n",
        "    gi = (A[i]@x - y[i])[0]*A[i] + (lambda_reg/N)*x\n",
        "    x = x - (gi/t)\n",
        "    # Update x using x <- x- 1/t * g_i (x)\n",
        "    t = t+1\n",
        "    if t>1e4:\n",
        "      t = 1\n",
        "algtime = timeit.default_timer()- start #time is in seconds\n",
        "x_opt = x\n",
        "print(\"Total Epochs is \", epochs)\n",
        "print(\"Time Taken is \", algtime)\n",
        "print(\"Norm of Gradient at x opt is\", norm(grad_fx(x_opt, lambda_reg)))\n",
        "print(\"||Ax_alg- y||^2  is\",norm(A@x - y)**2)\n",
        "print(\"||x_opt- xorig||^2 is   \", norm(x_opt - xorig.flatten())**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBhtUfovnM-Q",
        "outputId": "c5661181-8ba1-4a28-d275-18978f59d5ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epochs is  10000\n",
            "Time Taken is  350.0029364310001\n",
            "Norm of Gradient at x opt is 0.014280438515986649\n",
            "||Ax_alg- y||^2  is 6274940.964338467\n",
            "||x_opt- xorig||^2 is    19847.9190882912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**\n",
        "\n",
        "Time taken= 350.00293643100015 sec\n",
        "\n",
        "Norm of gradient at x_opt = 0.014280438515986649\n",
        "\n",
        "||Ax_alg- y||^2 = 6274940.964338467\n",
        "\n",
        "||x_opt- xorig||^2 :    19847.919033137703"
      ],
      "metadata": {
        "id": "RSrMqR4wbQ5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Fix λ = 0.001 and repeat the experiment for 106, 108 amd 1010 epochs and report the time taken for ALG-LAB6,\n",
        "∥∇fλ(x∗)∥, ∥Ax∗ − y∥22\n",
        "and ∥x∗ − xorig∥22\n",
        ". Explain your observations."
      ],
      "metadata": {
        "id": "HLfsRSJDZdfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epc = np.array([10**3, 10**5], dtype = int)\n",
        "for epoc in epc:\n",
        "  x = np.zeros((d,1))\n",
        "  x = x.flatten()\n",
        "  t = 1\n",
        "  arr = np.arange(N)\n",
        "  start = timeit.default_timer()\n",
        "  for epoch in range(epoc):\n",
        "    # print(\"epoch no.: \", epoch)\n",
        "    np.random.shuffle(arr) #shuffle every epoch\n",
        "    for i in np.nditer(arr): #Pass through the data points\n",
        "      gi = (A[i]@x - y[i])[0]*A[i] + (lambda_reg/N)*x\n",
        "      x = x - (gi/t)\n",
        "      # Update x using x <- x- 1/t * g_i (x)\n",
        "      t = t+1\n",
        "      if t>1e4:\n",
        "        t = 1\n",
        "  algtime = timeit.default_timer()- start #time is in seconds\n",
        "  x_opt = x\n",
        "  print(\"Total Epochs: \", epoc)\n",
        "  print(\"Time Taken: \", algtime)\n",
        "  print(\"Norm of Gradient at x-opt : \", norm(grad_fx(x_opt, lambda_reg)))\n",
        "  print(\"||Ax_alglab6- y||^2  : \",norm(A@x - y)**2)\n",
        "  print(\"||x_opt- xorig||^2 :   \", norm(x_opt - xorig.flatten())**2)\n",
        "  #print the time taken, ||Ax_alglab6- y||^2, ||x_opt- xorig||^2"
      ],
      "metadata": {
        "id": "WWOuyduSRPSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90949b52-8c77-4368-ff3b-b1ad311fbf71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epochs:  1000\n",
            "Time Taken:  37.17554169899995\n",
            "Norm of Gradient at x-opt :  0.01647208488895905\n",
            "||Ax_alglab6- y||^2  :  6274945.118619619\n",
            "||x_opt- xorig||^2 :    19847.919066320854\n",
            "Total Epochs:  100000\n",
            "Time Taken:  3434.3294305740005\n",
            "Norm of Gradient at x-opt :  0.01990010104843626\n",
            "||Ax_alglab6- y||^2  :  6274946.206369429\n",
            "||x_opt- xorig||^2 :    19847.919111883606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "Increasing the total number of epochs from 1000 to 100000 significantly increases the time taken for optimization, from 37.18 seconds to 3434.33 seconds, indicating a substantial increase in computational cost with more epochs.\n",
        "\n",
        "The norm of the gradient at the optimal solution slightly increases from 0.0165 to 0.0199 when the number of epochs increases, indicating a potentially slower convergence rate with a larger number of epochs.\n",
        "\n",
        "The values of ||Ax_alglab6- y||^2 and ||x_opt- xorig||^2 remain almost unchanged between the two cases, suggesting that the difference between the predicted output and actual output, as well as the difference between the optimal solution and the original solution, is not significantly affected by the number of epochs beyond 1000."
      ],
      "metadata": {
        "id": "_aXl0oln3CTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** I have taken only $10^3,10^5$ of epoch values becuase TA's tell us to take these values"
      ],
      "metadata": {
        "id": "nj0JRqrlZg40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Fix 109 and try λ ∈ {1000, 100, 10, 1, 0.1, 10−2, 10−3} and report the time taken for ALG-LAB6 ∥∇fλ(x∗)∥,\n",
        "∥Ax∗ − y∥22\n",
        "and ∥x∗ − xorig∥22\n",
        ". Explain your observations."
      ],
      "metadata": {
        "id": "0hzfk8ngZ5o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lamdas = [10**3, 10**2, 10, 1, 0.1, 0.01, 0.001]\n",
        "epochs = 10**4\n",
        "\n",
        "for lamda in lamdas:\n",
        "  x = np.zeros((d,1))\n",
        "  x = x.flatten()\n",
        "  t = 1\n",
        "  arr = np.arange(N) #index array\n",
        "  start = timeit.default_timer() #start the timer\n",
        "  for epoch in range(epochs):\n",
        "    # print(\"epoch no.: \", epoch)\n",
        "    np.random.shuffle(arr) #shuffle every epoch\n",
        "    for i in np.nditer(arr): #Pass through the data points\n",
        "      gi = (A[i]@x - y[i])[0]*A[i] + (lamda/N)*x\n",
        "      x = x - (gi/t)\n",
        "      # Update x using x <- x- 1/t * g_i (x)\n",
        "      t = t+1\n",
        "      if t>1e4:\n",
        "        t = 1\n",
        "\n",
        "  algtime = timeit.default_timer()- start #time is in seconds\n",
        "  x_opt = x\n",
        "  print(\"Total Epochs: is \", epochs)\n",
        "  print(\"Lambda Regularizer taken is \", lamda)\n",
        "  print(\"Time Taken is  \", algtime)\n",
        "  print(\"Norm of Gradient at x_opt is \", norm(grad_fx(x_opt, lamda)))\n",
        "  print(\"||Ax_alg- y||^2  is \",norm(A@x - y)**2)\n",
        "  print(\"||x_opt- xorig||^2 is   \", norm(x_opt - xorig.flatten())**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSuFh6dYAGZw",
        "outputId": "9aa03d01-127e-442d-ab38-17037b153d1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  1000\n",
            "Time Taken is   377.43905466600154\n",
            "Norm of Gradient at x_opt is  11.030815456470403\n",
            "||Ax_alg- y||^2  is  3168743.0548190055\n",
            "||x_opt- xorig||^2 is    19972.932410778765\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  100\n",
            "Time Taken is   355.99423169799957\n",
            "Norm of Gradient at x_opt is  37.008674573943516\n",
            "||Ax_alg- y||^2  is  3974633.1820549257\n",
            "||x_opt- xorig||^2 is    19883.18775236549\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  10\n",
            "Time Taken is   408.64287357600006\n",
            "Norm of Gradient at x_opt is  80.24782693457189\n",
            "||Ax_alg- y||^2  is  5837626.342537512\n",
            "||x_opt- xorig||^2 is    19848.752893582514\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  1\n",
            "Time Taken is   347.7240635440012\n",
            "Norm of Gradient at x_opt is  15.675638760255639\n",
            "||Ax_alg- y||^2  is  6235253.438350475\n",
            "||x_opt- xorig||^2 is    19847.955216602193\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  0.1\n",
            "Time Taken is   342.2555698979995\n",
            "Norm of Gradient at x_opt is  1.8289074460341974\n",
            "||Ax_alg- y||^2  is  6272100.496159463\n",
            "||x_opt- xorig||^2 is    19847.91159254104\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  0.01\n",
            "Time Taken is   356.9129988709992\n",
            "Norm of Gradient at x_opt is  0.1463513552509816\n",
            "||Ax_alg- y||^2  is  6274591.229794395\n",
            "||x_opt- xorig||^2 is    19847.91909791031\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  0.001\n",
            "Time Taken is   347.09561961800136\n",
            "Norm of Gradient at x_opt is  0.017582094055818884\n",
            "||Ax_alg- y||^2  is  6274934.92373323\n",
            "||x_opt- xorig||^2 is    19847.919111523977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the results, as the regularization parameter λ decreases, the norm of the gradient at the optimum also decreases, indicating better convergence. However, reducing λ too much can lead to overfitting, as seen in the increase of ||Ax_alg- y||^2. The time taken generally increases with smaller λ due to more iterations needed for convergence. Balancing between regularization strength, convergence speed, and model fit is crucial. Despite differences in regularization, the final ||x_opt- xorig||^2 remains relatively stable, suggesting the regularization impact on the solution's closeness to the original."
      ],
      "metadata": {
        "id": "aeqHg3JN_gBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Does ALG-LAB6 work for failure dimension?"
      ],
      "metadata": {
        "id": "mtO3p43R4tHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  Yes alg-Lab6 works for the failure dimention of last question.In the last question failure occure at 20000 so I have only check at that point but not for large values of d"
      ],
      "metadata": {
        "id": "a9Kcofg64uNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain your understanding of ALG-LAB6."
      ],
      "metadata": {
        "id": "z9FVU7nB5UZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "ALG-LAB6 appears to be a stochastic optimization algorithm, likely a variant of stochastic gradient descent (SGD) due to its per-sample update rule.\n",
        "\n",
        "It aims to minimize the difference between the predicted output Ax_alglab6 and the actual output y by updating the optimization variable x iteratively.\n",
        "The algorithm employs a learning rate schedule where the learning rate decreases over time as 1/t, where t is the iteration counter.\n",
        "\n",
        "The shuffling of data points in each epoch helps in preventing the algorithm from getting stuck in local minima and aids in better exploration of the solution space.\n",
        "\n",
        "The reported metrics such as time taken, squared norm of residual, and squared norm of difference provide insights into the performance and accuracy of the algorithm."
      ],
      "metadata": {
        "id": "6BBn8kKf5VXy"
      }
    }
  ]
}